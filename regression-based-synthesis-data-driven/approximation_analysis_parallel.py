"""By running this code, the maximum error between the continuous disks generated by NNs and analytical method will be
computed and saved."""
import os
import numpy as np
import math
import time
import ray
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense
from tensorflow.python.keras.layers import deserialize, serialize
from tensorflow.python.keras.saving import saving_utils
import env_pool


def unpack(model, training_config, weights):
    restored_model = deserialize(model)
    if training_config is not None:
        restored_model.compile(
            **saving_utils.compile_args_from_training_config(
                training_config
            )
        )
    restored_model.set_weights(weights)
    return restored_model

# Hotfix function
def make_keras_picklable():

    def __reduce__(self):
        model_metadata = saving_utils.model_metadata(self)
        training_config = model_metadata.get("training_config", None)
        model = serialize(self)
        weights = self.get_weights()
        return (unpack, (model, training_config, weights))

    cls = Model
    cls.__reduce__ = __reduce__


def discrete_sys_size_gen():
    """ This function computes a vector that contains number of
     discrete states for every dimension of state and input spaces."""
    discrete_sys_size = np.zeros(dim_x + dim_u)
    for ii in range(0, dim_x):
        discrete_sys_size[ii] = math.floor((X_range[ii, 1] - X_range[ii, 0] - eta_x[ii]) / eta_x[ii] + 1)
    for ii in range(dim_x, dim_x + dim_u):
        discrete_sys_size[ii] = math.floor(
            (U_range[ii - dim_x, 1] - U_range[ii - dim_x, 0] - eta_u[ii - dim_x]) / eta_u[ii - dim_x] + 1)
    return discrete_sys_size.astype(int)


def NN_structure_TS():
    """Create the model for estimating the transition system."""
    model = Sequential()
    model.add(Dense(40, input_dim=dim_x+dim_u, activation=tf.nn.tanh))
    model.add(Dense(80, activation=tf.nn.tanh))
    model.add(Dense(35, activation=tf.nn.tanh))
    model.add(Dense(60, activation=tf.nn.tanh))
    model.add(Dense(dim_x, activation='linear'))
    return model


def approximation_error_TS(inp_filename, out_filename_c, out_filename_r, err_vec_filename, num_samples, NN_c, NN_r, inp_dim, out_dim):
    e_vec = np.memmap(err_vec_filename, dtype='float32', mode='w+',
                           shape=(num_samples // length+1, out_dim))
    x_tr = np.memmap(inp_filename, dtype='float32', mode='r', shape=(num_samples, inp_dim), offset=0)
    y_tr_c = np.memmap(out_filename_c, dtype='float32', mode='r', shape=(num_samples, out_dim), offset=0)
    y_tr_r = np.memmap(out_filename_r, dtype='float32', mode='r', shape=(num_samples, out_dim), offset=0)
    for step in range(num_samples // num_tasks_per_step+1):
        offset = step*num_tasks_per_step
        num_tasks_in_this_step = min(num_tasks_per_step, num_samples - step * num_tasks_per_step)
        e_coded = [single_point_error_TS.remote(x_tr[offset+i*length:min(offset+(i+1)*length, offset+num_tasks_in_this_step)],
                                                  y_tr_c[offset+i*length:min(offset+(i+1)*length, offset+num_tasks_in_this_step)],
                                                  y_tr_r[offset + i * length:min(offset + (i + 1) * length, offset + num_tasks_in_this_step)],
                                                  NN_c, NN_r, num_tasks_in_this_step-i*length) for i in range(0, num_tasks_in_this_step // length+1)]
        e_decoded = ray.get(e_coded)
        for i in range(num_tasks_in_this_step // length+1):
            if min(length, num_tasks_in_this_step - i * length) != 0:
                e_vec[step * num_tasks_per_step//length + i, :] = e_decoded[i]
    mismatch_error = e_vec.max(axis=0)
    return mismatch_error

@ray.remote#(num_returns=2)
def single_point_error_TS(inp_dataset, out_dataset_c, out_dataset_r, NN_c, NN_r, num_tasks):
    length_for_this_step = min(length, num_tasks)
    error = np.zeros(dim_x)
    for i in range(length_for_this_step):
        ind = i  # ii*length+i
        NN_inp = inp_dataset[ind, :]  # for both NN_c and NN_r
        c = out_dataset_c[ind, :]
        r = out_dataset_r[ind, :]
        c_NN =np.squeeze(NN_c(np.array([NN_inp])).numpy())
        r_NN = np.squeeze(NN_r(np.array([NN_inp])).numpy())
        e1 = np.zeros(dim_x)
        e2 = np.zeros(dim_x)
        if (c+r-(c_NN+r_NN) > 0).any():  # check if the up-right corner is inside
            e1 = c+r-(c_NN+r_NN)
        if (c-r-(c_NN-r_NN) < 0).any():  # check if the down-left corner is inside
            e2 = (c_NN-r_NN)-(c-r)
        this_step_error = np.maximum(e1, e2)
        error = np.maximum(error, this_step_error)

    return error

def approximation_error_GB(inp_filename, out_filename, err_vec_filename, num_samples, NN, inp_dim, out_dim):
    e_vec = np.memmap(err_vec_filename, dtype='float32', mode='w+',
                           shape=(num_samples // length+1, out_dim))
    x_tr = np.memmap(inp_filename, dtype='float32', mode='r', shape=(num_samples, inp_dim), offset=0)
    y_tr = np.memmap(out_filename, dtype='float32', mode='r', shape=(num_samples, out_dim), offset=0)
    # e_vec = np.memmap(err_vec_filename, dtype='float32', mode='w+', shape=(num_samples, out_dim), offset=0)
    # e_vec.flush()
    # error = np.zeros(dim_x)
    for step in range(num_samples // num_tasks_per_step+1):
        offset = step*num_tasks_per_step
        num_tasks_in_this_step = min(num_tasks_per_step, num_samples - step * num_tasks_per_step)
        #  start = time.time()
        e_decoded = [single_point_error_GB(x_tr[offset+i*length:min(offset+(i+1)*length, offset+num_tasks_in_this_step)],
                                                  y_tr[offset+i*length:min(offset+(i+1)*length, offset+num_tasks_in_this_step)],
                                                  NN, num_tasks_in_this_step-i*length) for i in range(0, num_tasks_in_this_step // length+1)]
        for i in range(num_tasks_in_this_step // length+1):
            if min(length, num_tasks_in_this_step - i * length) != 0:
                e_vec[step * num_tasks_per_step//length + i, :] = e_decoded[i]
    mismatch_error = e_vec.max(axis=0)
    return mismatch_error

def single_point_error_GB(inp_dataset, out_dataset, NN, num_tasks):
    length_for_this_step = min(length, num_tasks)
    error = np.zeros(dim_x)
    for i in range(length_for_this_step):
        ind = i  # ii*length+i
        NN_inp = abs(inp_dataset[ind, :])  # x_tr = np.memmap(inp_filename, dtype='float32', mode='r+', shape=(1, dim_x + dim_u), offset=int(ind * (dim_x + dim_u) * 32 / 8))
        y_analytical = out_dataset[ind, :]  # = np.memmap(out_filename, dtype='float32', mode='r+', shape=(1, dim_x), offset=int(ind * dim_x * 32 / 8))
        # error_new = abs(y_tr - NN_forw(x_tr))
        error = np.maximum(error, abs(y_analytical-NN(np.array([NN_inp]))))
    return error


# Setting the parameters
env = env_pool.inv_pend()

X_range = env.X_range  # state-space
U_range = env.U_range  # input space
sample_time = env.sample_time  # sampling time in seconds
eta_x = env.eta_x  # state-space discretization size
eta_u = env.eta_u  # input-space discretization size
W = env.W  # disturbance bounds
nbins = env.nbins  # used for computing the integral
time_span = env.time_span  # used for computing the integral
# parallelization parameters
length = env.length
num_tasks_per_step = env.num_tasks_per_step
# defining filenames for saving the transition system
"""forw_sub_inp_TS_filename = env.forw_sub_inp_TS_filename
forw_sub_out_TS_c_filename = env.forw_sub_out_TS_c_filename
forw_sub_out_TS_r_filename = env.forw_sub_out_TS_r_filename"""
forw_sub_inp_TS_filename = env.forw_inp_TS_filename
forw_sub_out_TS_c_filename = env.forw_out_TS_c_filename
forw_sub_out_TS_r_filename = env.forw_out_TS_r_filename
back_inp_TS_filename = env.back_inp_TS_filename
back_out_TS_c_filename = env.back_out_TS_c_filename
back_out_TS_r_filename = env.back_out_TS_r_filename
# defining paths for saving the trained NNs
checkpoint_path_TS_forw = env.checkpoint_path_TS_forw
checkpoint_path_TS_back = env.checkpoint_path_TS_back
checkpoint_path_GB_forw = env.checkpoint_path_GB_forw
checkpoint_path_GB_back = env.checkpoint_path_GB_back
# defining filenames for saving the approximation error vector
e_vec_TS_forw_filename = env.e_vec_TS_forw_filename
e_vec_TS_back_filename = env.e_vec_TS_back_filename
e_vec_GB_forw_filename = env.e_vec_GB_forw_filename
e_vec_GB_back_filename = env.e_vec_GB_back_filename
# defining filenames for saving the safety margins
safety_margin_FW_filename = env.safety_margin_FW_filename
safety_margin_BW_filename = env.safety_margin_BW_filename

# Extract descriptive parameters of the system
rr_x = eta_x / 2  # radius of the partitions in the state-space
rr_u = eta_u / 2  # radius of the partitions in the input-space
dim_x = np.shape(X_range)[0]  # dimension of the state-space
dim_u = np.shape(U_range)[0]  # dimension of the input-space
discrete_sys_size = discrete_sys_size_gen()  # vector containing number of discrete pointsalong each dimension in the
# state and input spaces
num_dis_states = np.prod(discrete_sys_size[0:dim_x]).astype(int)  # size of the state-space
num_dis_inputs = np.prod(discrete_sys_size[dim_x:dim_x + dim_u]).astype(int)  # size of the input-space
num_state_inp_pairs = np.prod(discrete_sys_size).astype(int)  # number of state-input pairs


# Run the function
make_keras_picklable()
# Load the model
NN_TS_forw = NN_structure_TS()
NN_TS_back = NN_structure_TS()
NN_GB_forw = NN_structure_TS()
NN_GB_back = NN_structure_TS()
# Loads the weights
NN_TS_forw.load_weights(checkpoint_path_TS_forw)
NN_TS_back.load_weights(checkpoint_path_TS_back)

NN_GB_forw.load_weights(checkpoint_path_GB_forw)
NN_GB_back.load_weights(checkpoint_path_GB_back)


# Computing the approximation error for different NNs
start = time.time()
ray.init(_plasma_directory="/tmp", log_to_driver=False)
e_vec_TS_forw = approximation_error_TS(forw_sub_inp_TS_filename, forw_sub_out_TS_c_filename, forw_sub_out_TS_r_filename, e_vec_TS_forw_filename, num_state_inp_pairs, NN_TS_forw, NN_GB_forw, dim_x+dim_u, dim_x)
print("Maximum forward TS approximation error is", e_vec_TS_forw)
print("Execution time for forward TS approximation error computation is", time.time() - start)
ray.shutdown()
ray.init(_plasma_directory="/tmp", log_to_driver=False)
e_vec_TS_back = approximation_error_TS(back_inp_TS_filename, back_out_TS_c_filename, back_out_TS_r_filename, e_vec_TS_back_filename, num_state_inp_pairs, NN_TS_back, NN_GB_back, dim_x+dim_u, dim_x)
print("Maximum backward TS approximation error is", e_vec_TS_back)

#  e_vec_GB_forw = approximation_error_TS(forw_sub_inp_TS_filename, forw_sub_out_TS_r_filename, e_vec_GB_forw_filename, num_state_inp_pairs, NN_GB_forw, dim_x+dim_u, dim_x)
e_vec_GB_forw = np.zeros(dim_x)
print("Maximum forward GB approximation error is", e_vec_GB_forw)

#  e_vec_GB_back = approximation_error_TS(back_inp_TS_filename, back_out_TS_r_filename, e_vec_GB_back_filename, num_state_inp_pairs, NN_GB_back, dim_x+dim_u, dim_x)
e_vec_GB_back = np.zeros(dim_x)
print("Maximum backward GB approximation error is", e_vec_GB_back)
ray.shutdown()
print("Total execution time is", time.time() - start)

safety_margin_forw = e_vec_TS_forw+e_vec_GB_forw
safety_margin_back = e_vec_TS_back+e_vec_GB_back
np.save(safety_margin_FW_filename, safety_margin_forw)
np.save(safety_margin_BW_filename, safety_margin_back)


